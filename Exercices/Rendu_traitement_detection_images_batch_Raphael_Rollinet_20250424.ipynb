{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e01997c7",
   "metadata": {},
   "source": [
    "# Application de methode de deep learning à un fonds d'images d'archives\n",
    "\n",
    "## Du pixel aux images - 32M7138\n",
    "\n",
    "*Printemps 2025 - Université de Genève*\n",
    "\n",
    "*Raphaël Rollinet (raphael.rollinet@unine.ch)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f70e97b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Ce Markdown effectué dans le cadre du cours \"Du pixel aux images : introduction au traitement des images 2D\" traite de méthode de deep learning appliqué au traitement de l'image, plus précisément de la détection d'objet sur un fonds d'image ancienne. \n",
    "\n",
    "### Méthode de détection d'objet\n",
    "\n",
    "La méthode utilisée dans le cadre de ce projet est la suivante\n",
    "\n",
    "### Fonds d'images de la commune de Milvignes\n",
    "\n",
    "![alt text](Milvignes-blazon.svg)\n",
    "\n",
    "Le fonds d'images provient de l'inventaire des archives de la commune de Milvignes, membre du service intercommunal d'archives de Neuchâtel, la plateforme de diffusion des inventaires est gérée par docuteam SA, entreprise de gestion d'archives qui m'a employé de 2016 à 2021. Les images numérisées par mes soins en format de conservation (TIFF) et archivées électroniquement dans un repository de préservation numérique et également disponible  disponible en téléchargement librement dans un format dégradé de diffusion (JPEG). \n",
    "\n",
    "Les images sont disponibles ici : https://milvignes.docuteam.cloud/fr/units/1-archives-communales-de-milvignes-1326-2013/gallery\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f33300",
   "metadata": {},
   "source": [
    "## Problématique\n",
    "\n",
    "Ayant longuement travaillé dans le domaine des archives, la thématique d'appliquer les méthodes vues dans le cadre de ce cours sur un fonds d'archives est un sujet intéressant dans son potentiel d'application sur un fonds d'archives d'images. Dans les institutions patrimoniales, les images sont un support courant, ma problématique sera donc appliquée à un fonds ancien d'images numérisées de la commune de Milvignes (NE). Mon questionnement porte sur l'application de cette méthode utilisant des modèles de deep learning entrainé sur des images relativement récentes, mais ici appliqué à un fonds d'images anciennes (~1900).\n",
    "\n",
    "### Question de recherche\n",
    "\n",
    "- \"Quel est le potentiel de la détection d'objet dans le traitement de fonds d'images en archives ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ec479",
   "metadata": {},
   "source": [
    "## Méthodologie\n",
    "\n",
    "Le code utilisé est adapté du code vu en cours, il vise à effectuer une détection d'objet sur des images, tout en répondant à des contraintes métiers qui sont par exemple un besoin d'automatiser le traitement à partir d'un workflow. Mais également en termes de rendu, un archiviste aura besoin d'extraire les détections sous format CSV afin qu'elles soient intégrées aux notices d'un système d'information archivistique par exemple sous la forme d'une indexation. Le code permet à n'importe quel archiviste avec des notions informatiques minimales de l'exécuter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monter Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f26d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation des librairies (si nécessaire)\n",
    "!pip install opencv-python\n",
    "!pip install tensorflow_hub\n",
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87439cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des librairies Python\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e604ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les modèles (détection + classification)\n",
    "\n",
    "# Modèle de détection SSD MobileNet V2\n",
    "detector = hub.load(\"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1\")\n",
    "\n",
    "# Modèle ViT pour classification d'image\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "vit_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des noms de classes (80 classes COCO)\n",
    "classe = np.array([\n",
    "    'background', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "    'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
    "    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "    'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "    'hair drier', 'toothbrush'\n",
    "])\n",
    "classes = np.concatenate([classe, np.repeat('none', 100)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f91afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  À adapter : Mettre le dossier Images dans Google Drive\n",
    "folder_path = \"/content/drive/MyDrive/Colab_Notebooks/images\"  # Adapter le chemin selon l'utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc20cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour stocker les résultats\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de detection et classification pour le traitement de l'image\n",
    "## Traite une image avec SSD MobileNet (détection) et ViT (classification).\n",
    "## Sauvegarde l'image annotée et stocke les résultats dans la liste globale 'results'.\n",
    "\n",
    "def process_image_with_detection_and_classification(image_path):\n",
    "\n",
    "    # Lire l'image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Erreur : impossible de lire {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Convertir en RGB pour affichage et modèles\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    input_tensor = tf.convert_to_tensor(image_rgb)[tf.newaxis, ...]\n",
    "\n",
    "    # Détection d'objets\n",
    "    detections = detector(input_tensor)\n",
    "    boxes = detections['detection_boxes'][0].numpy()\n",
    "    scores = detections['detection_scores'][0].numpy()\n",
    "    detected_classes = detections['detection_classes'][0].numpy().astype(np.uint8)\n",
    "\n",
    "    # Préparer les annotations et objets détectés\n",
    "    object_labels = []\n",
    "    height, width, _ = image.shape\n",
    "    for i in range(len(scores)):\n",
    "        if scores[i] < 0.3:\n",
    "            continue\n",
    "        label = classes[detected_classes[i]]\n",
    "        object_labels.append(label)\n",
    "\n",
    "        # Boîte englobante\n",
    "        box = boxes[i] * np.array([height, width, height, width])\n",
    "        box = box.astype(np.int32)\n",
    "        cv2.rectangle(image_rgb, (box[1], box[0]), (box[3], box[2]), (0, 255, 0), 2)\n",
    "        cv2.putText(image_rgb, label, (box[1], box[0]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    # Classification avec ViT\n",
    "    pil_image = Image.fromarray(image_rgb)\n",
    "    inputs = processor(images=pil_image, return_tensors=\"pt\")\n",
    "    outputs = vit_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "    predicted_label = vit_model.config.id2label[predicted_class_idx]\n",
    "\n",
    "    # Sauvegarde de l’image annotée dans /Resultat/annotated\n",
    "    result_folder = os.path.join(folder_path, \"Resultat\")\n",
    "    annotated_folder = os.path.join(result_folder, \"annotated\")\n",
    "    os.makedirs(annotated_folder, exist_ok=True)\n",
    "\n",
    "    save_path = os.path.join(annotated_folder, os.path.basename(image_path))\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Affichage pour visualisation\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{os.path.basename(image_path)}\\n→ ViT : {predicted_label}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Stockage des résultats pour export CSV\n",
    "    results.append({\n",
    "        \"image\": os.path.basename(image_path),\n",
    "        \"detected_objects\": \", \".join(object_labels),\n",
    "        \"vit_prediction\": predicted_label\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b67aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer à toutes les images présentes dans le dossiers \"images\"\n",
    "supported_formats = ('.jpg', '.jpeg', '.png', '.bmp')\n",
    "image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(supported_formats)]\n",
    "\n",
    "print(f\"{len(image_files)} images trouvées.\")\n",
    "\n",
    "for image_path in image_files:\n",
    "    process_image_with_detection_and_classification(image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def87294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un sous-dossier \"Résultat\" dans le dossier Drive d'origine\n",
    "result_folder = os.path.join(folder_path, \"Resultat\")\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "\n",
    "# Définir le chemin de sortie du CSV dans ce dossier\n",
    "csv_output_path = os.path.join(result_folder, \"resultats_detection_classification.csv\")\n",
    "\n",
    "# Export du DataFrame en CSV\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(csv_output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"CSV enregistré ici : {csv_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1386b7",
   "metadata": {},
   "source": [
    "## Résultat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c9d53",
   "metadata": {},
   "source": [
    "## Mise en perspectives et conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753c2c6",
   "metadata": {},
   "source": [
    "## Bibliographie"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
